from tensorflow import keras
import sklearn as sklearn
from PIL import Image as im
import tensorflow as tf
from tensorflow.keras import datasets, layers, Sequential, models
import numpy as np
import cv2
from imutils.contours import sort_contours
import imutils

train_data_points = 800
test_data_points = 216
total_data_points = train_data_points + test_data_points
pixels = 50
bins = 62
mid_layer = 750
training_iters = 10
learning_rate = 1e-3
batch_size = 512
num_epochs = 3


class LearningRateReducerCb(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        old_lr = self.model.optimizer.lr.read_value()
        new_lr = old_lr * 0.5
        print("\nEpoch: {}. Reducing Learning Rate from {} to {}".format(epoch, old_lr, new_lr))
        self.model.optimizer.lr.assign(new_lr)


def convert_to_char(value):
    if -1 < value < 10:
        return chr(int(value))
    elif 9 < value < 36:
        return chr(int(value + 55))
    elif 35 < value < 62:
        return chr(int(value + 61))
    else:
        return '*'


images = []
# inPath = "C:/NN/NIST SD 19/DataSet/"
folder_path = "C:/NN/Dataset 4/"
for i in range(bins):
    for j in range(total_data_points):

        total_iterations = bins * total_data_points
        p = np.round(((i * total_data_points + j) / total_iterations) * 100, 2)
        print('\r''Data loading:', p, '%', end='')

        filename = folder_path + "img" + str(i + 1).zfill(2) + "-" + str(j + 1).zfill(4) + ".png"
        img = im.open(filename)
        img = img.resize((pixels, pixels))
        img = img.convert('L')
        img_array = np.asarray(img)
        images.append(img_array)
        img.close()
        img.close()

print('\n')
images_array = np.reshape(images, (bins, total_data_points, pixels, pixels))

training_data = images_array[:, 0:train_data_points, :, :]
training_data = np.reshape(training_data, (train_data_points * bins, pixels, pixels))

test_data = images_array[:, train_data_points:total_data_points, :, :]
test_data = np.reshape(test_data, (test_data_points * bins, pixels, pixels))

training_labels = np.empty(0)
test_labels = np.empty(0)
for i in range(bins):
    to_add = i * np.ones(train_data_points)
    training_labels = np.append(training_labels, to_add)
    to_add = i * np.ones(test_data_points)
    test_labels = np.append(test_labels, to_add)

# training_data, training_labels = sklearn.utils.shuffle(training_data, training_labels)
# test_data, test_labels = sklearn.utils.shuffle(test_data, test_labels)

training_data = training_data / 255.0
test_data = test_data / 255.0

model = models.Sequential()
model.add(layers.Conv2D(pixels, (3, 3), activation='relu', input_shape=(pixels, pixels, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(2 * pixels, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(2 * pixels, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())
model.add(layers.Dense(2 * pixels, activation='relu'))
model.add(layers.Dense(62))

opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=opt,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(training_data, training_labels, callbacks=[LearningRateReducerCb()], epochs=num_epochs,
                    validation_data=(test_data, test_labels))

test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)
print("Accuracy = " + str(np.round(test_acc * 100, 3)) + " %")

weights = model.get_weights()

# wt = open('C:/NN/NIST SD 19/Weights, Biases/weights.csv', 'w')
# bs = open('C:/NN/NIST SD 19/Weights, Biases/biases.csv', 'w')

gray = cv2.imread("C:/NN/NIST SD 19/Words/Test 0.jpg", 0)
# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
blurred = cv2.GaussianBlur(gray, (5, 5), 0)
edged = cv2.Canny(blurred, 100, 200)
# cv2.imshow("edged", edged)
# cv2.waitKey(0)
cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
cnts = imutils.grab_contours(cnts)
cnts = sort_contours(cnts, method="left-to-right")[0]

chars = ""

print("Now predicting")
iteration = 0
for c in cnts:
    iteration = iteration + 1
    (x, y, w, h) = cv2.boundingRect(c)
    if (w >= 5 and w <= 150) and (h >= 15 and h <= 120):
        roi = gray[y:y + h, x:x + w]

        thresh = cv2.threshold(roi, 0, 255,
                               cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
        (tH, tW) = thresh.shape
        if tW > tH:
            thresh = imutils.resize(thresh, width=32)
        else:
            thresh = imutils.resize(thresh, height=32)

        (tH, tW) = thresh.shape
        dX = int(max(0, 32 - tW) / 2.0)
        dY = int(max(0, 32 - tH) / 2.0)
        padded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,
                                    left=dX, right=dX, borderType=cv2.BORDER_CONSTANT,
                                    value=(0, 0, 0))
        padded = cv2.resize(padded, (pixels, pixels))
        # padded = cv2.copyMakeBorder(padded, 0.2 * pixels, 0.2 * pixels, 0.2 * pixels, 0.2 * pixels, cv2.BORDER_CONSTANT, value=[0, 0, 0])
        padded = ~padded
        padded = padded.astype("float32") / 255.0
        padded = np.expand_dims(padded, axis=-1)
        cv2.imshow("Char", padded)
        cv2.waitKey(0)
        print(np.shape(padded))
        padded = np.expand_dims(padded, axis=0)
        predictions = model.predict(padded)
        print(np.argmax(predictions))
        print(convert_to_char(np.argmax(predictions)))
        chars = chars + convert_to_char(np.argmax(predictions))

print(chars)
# for idx, i in enumerate(model.layers):
#     if isinstance(i, layers.Conv2D) or isinstance(i, layers.Dense):
#         weights = i.get_weights()[0]
#         biases = i.get_weights()[1]
#         weights = weights.flatten()
#         biases = biases.flatten()
#         print("weights: ", weights.shape)
#         print("biases: ", biases.shape)
#         wt.write(','.join(map(str, weights.tolist()))+"\n")
#         bs.write(','.join(map(str, biases.tolist()))+"\n")

# New method starts

# model = keras.Sequential([
#     keras.layers.Flatten(input_shape=(pixels, pixels)),
#     keras.layers.Dense(mid_layer, activation='relu'),
#     keras.layers.Dense(bins, activation='softmax')
# ])
#
# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
#
# history = model.fit(training_data, training_labels, epochs=10,
#                     validation_data=(test_data, test_labels))
#
# test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)
# print(str(test_acc * 100) + "  %")
