from tensorflow import keras
import sklearn as sklearn
from PIL import Image as im
import tensorflow as tf
from tensorflow.keras import datasets, layers, Sequential, models
import numpy as np

train_data_points = 800
test_data_points = 200
total_data_points = train_data_points + test_data_points
pixels = 50
bins = 62
mid_layer = 750
training_iters = 10
learning_rate = 0.005
batch_size = 512


class LearningRateReducerCb(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        old_lr = self.model.optimizer.lr.read_value()
        new_lr = old_lr * 0.5
        print("\nEpoch: {}. Reducing Learning Rate from {} to {}".format(epoch, old_lr, new_lr))
        self.model.optimizer.lr.assign(new_lr)


def convert_to_char(value):
    if -1 < value < 10:
        return int(value)
    if 9 < value < 36:
        return chr(int(value + 55))
    if 35 < value < 62:
        return chr(int(value + 61))


images = []
inPath = "C:/NN/NIST SD 19/DataSet/"
i = 0
for i in range(48, 123):
    p = np.round(((i - 48) / 74) * 100, 2)
    print('\r''Data loading:', p, '%', end='')
    for j in range(total_data_points):
        if 47 < i < 58 or 64 < i < 91 or 96 < i < 123:
            input_path = inPath + str(i) + '-' + str(j) + '.png'
            img = im.open(input_path)
            img = img.resize((pixels, pixels))
            img = img.convert('L')
            img_array = np.asarray(img)
            images.append(img_array)
            img.close()

print('\n')
images_array = np.reshape(images, (bins, total_data_points, pixels, pixels))

training_data = images_array[:, 0:train_data_points, :, :]
training_data = np.reshape(training_data, (train_data_points * bins, pixels, pixels))

test_data = images_array[:, train_data_points:total_data_points, :, :]
test_data = np.reshape(test_data, (test_data_points * bins, pixels, pixels))

training_labels = np.empty(0)
test_labels = np.empty(0)
for i in range(bins):
    to_add = i * np.ones(train_data_points)
    training_labels = np.append(training_labels, to_add)
    to_add = i * np.ones(test_data_points)
    test_labels = np.append(test_labels, to_add)

training_data, training_labels = sklearn.utils.shuffle(training_data, training_labels)
test_data, test_labels = sklearn.utils.shuffle(test_data, test_labels)

training_data = training_data / 255.0
test_data = test_data / 255.0

model = models.Sequential()
model.add(layers.Conv2D(pixels, (3, 3), activation='relu', input_shape=(pixels, pixels, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(2 * pixels, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
# model.add(layers.Conv2D(2 * pixels, (3, 3), activation='relu'))
model.summary()

model.add(layers.Flatten())
model.add(layers.Dense(2 * pixels, activation='relu'))
model.add(layers.Dense(62))
model.summary()

opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=opt,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(training_data, training_labels, callbacks=[LearningRateReducerCb()], epochs=3,
                    validation_data=(test_data, test_labels))

test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)
print("Accuracy = " + str(np.round(test_acc * 100, 3)) + " %")

wt = open('C:/NN/NIST SD 19/Weights, Biases/weights.csv', 'w')
bs = open('C:/NN/NIST SD 19/Weights, Biases/biases.csv', 'w')

for idx, i in enumerate(model.layers):
    if isinstance(i, layers.Conv2D) or isinstance(i, layers.Dense):
        weights = i.get_weights()[0]
        biases = i.get_weights()[1]
        weights = weights.flatten()
        biases = biases.flatten()
        print("weights: ", weights.shape)
        print("biases: ", biases.shape)
        wt.write(','.join(map(str, weights.tolist()))+"\n")
        bs.write(','.join(map(str, biases.tolist()))+"\n")

# for i in range(7):
#     print(model.layers[i].weights)
#     print(model.layers[i].bias.numpy())
#     print(model.layers[i].bias_initializer)

# New method starts

# model = keras.Sequential([
#     keras.layers.Flatten(input_shape=(pixels, pixels)),
#     keras.layers.Dense(mid_layer, activation='relu'),
#     keras.layers.Dense(bins, activation='softmax')
# ])
#
# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
#
# history = model.fit(training_data, training_labels, epochs=10,
#                     validation_data=(test_data, test_labels))
#
# test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)
# print(str(test_acc * 100) + "  %")
